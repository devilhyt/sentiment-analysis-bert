{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合併所有看板，並刪除字數大於510的文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "r_dir = './data/'\n",
    "w_dir = './ptt_dataset/'\n",
    "board_list = [\"happy\", \"hate\", \"sad\"]\n",
    "MAX_LENGTH = 510\n",
    "\n",
    "# 載入並合併所有心情看板的檔案\n",
    "dfs = [pd.read_csv(f\"{r_dir}{f}.csv\") for f in board_list]\n",
    "df_train = pd.concat(dfs)\n",
    "\n",
    "# 過濾字數大於510字的文章，並存檔\n",
    "df_train = df_train[~(df_train.content.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "df_train.to_csv(f\"{w_dir}dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割dataset，訓練集、測試集、驗證集的比例為8:1:1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rw_dir = './ptt_dataset/'\n",
    "random_state = 1410832008\n",
    "\n",
    "df = pd.read_csv(f\"{rw_dir}dataset.csv\")\n",
    "df_y = df['board']\n",
    "df_x = df\n",
    "\n",
    "# 訓練集：測試集：驗證集（8:1:1）\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_x, df_y, stratify=df_y, test_size=0.2, random_state=random_state)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, stratify=y_test, test_size=0.5, random_state=random_state)\n",
    "\n",
    "# 分別存成 .csv\n",
    "X_train.to_csv(f\"{rw_dir}train.csv\", index=False)\n",
    "X_test.to_csv(f\"{rw_dir}test.csv\", index=False)\n",
    "X_val.to_csv(f\"{rw_dir}validation.csv\", index=False)\n",
    "\n",
    "print(pd.Series(df_y).value_counts(normalize=True))\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(pd.Series(y_test).value_counts(normalize=True))\n",
    "print(pd.Series(y_val).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "預處理\n",
    "1. 將資料集轉成Transformers datasets格式\n",
    "2. 載入bert Tokenizer並斷詞\n",
    "3. 定義每一種心情的label數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ed1cb8157bbbb21b\n",
      "Reusing dataset csv (C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25638b7600e8471ebdc391de3625d600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-54a7a3cf882b552d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-267e4a2abd944cf2.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-da341eebc47c18c6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-9af0cff25888864e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-826d3e96e468ee14.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-8907c4c0be007752.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-6a94ab90b274d3cd.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-a55d9a455ac03624.arrow\n",
      "Loading cached processed dataset at C:\\Users\\DevilHYT\\.cache\\huggingface\\datasets\\csv\\default-ed1cb8157bbbb21b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-318fcb9ed95a0083.arrow\n"
     ]
    }
   ],
   "source": [
    "\"\"\"預處理\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 將資料集轉成Transformers datasets格式\n",
    "r_dir = './ptt_dataset/'\n",
    "\n",
    "data_files = {\"train\": f\"{r_dir}train.csv\",\n",
    "              \"test\": f\"{r_dir}test.csv\", \"validation\": f\"{r_dir}validation.csv\"}\n",
    "raw_datasets = load_dataset(\"csv\", data_files=data_files, sep=\",\")\n",
    "\n",
    "# 載入 bert Tokenizer\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# 斷詞 function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"content\"], truncation=True)\n",
    "\n",
    "# 斷詞\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"content\"])\n",
    "\n",
    "# 定義每一種心情的label數值 \n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"board\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.class_encode_column(\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入用於Padding的Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入bert分類模型，調整Hyper-parameter，將其移到GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3,\n",
    "    classifier_dropout=0.4,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1\n",
    "    ).to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義計算準確度的Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric_accuracy = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric_accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定Transformers Trainer的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "strategy = \"epoch\"\n",
    "steps = 90\n",
    "dir = \"bert-base-chinese-20220610-5\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=dir,\n",
    "                                  evaluation_strategy=strategy,\n",
    "                                  eval_steps=steps,\n",
    "                                  save_strategy=strategy,\n",
    "                                  save_steps=steps,\n",
    "                                  logging_strategy=strategy,\n",
    "                                  logging_steps=steps,\n",
    "                                  # logging_first_step=True,\n",
    "                                  report_to=\"wandb\",\n",
    "                                  run_name=\"bert-base-chinese\",\n",
    "                                  group_by_length=True,\n",
    "                                  learning_rate=2e-6,\n",
    "                                  num_train_epochs=10,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  per_device_eval_batch_size=16,\n",
    "                                  gradient_accumulation_steps=4,\n",
    "                                  gradient_checkpointing=True,\n",
    "                                  lr_scheduler_type=\"linear\",\n",
    "                                  warmup_ratio=0.3,\n",
    "                                  # weight_decay=0.01,\n",
    "                                  fp16=True,\n",
    "                                  load_best_model_at_end=True\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立Trainer的實例，並指定：\n",
    "1. 使用的模型\n",
    "2. Trainer參數\n",
    "3. 資料集\n",
    "4. padding的Data Collator\n",
    "5. 斷詞器\n",
    "6. 計算準確度的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DevilHYT\\miniconda3\\envs\\bert\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11438\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1780\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevilhyt\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DevilHYT\\Desktop\\github\\sentiment-classification-bert\\wandb\\run-20220610_231434-b0vnjudo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/devilhyt/huggingface/runs/b0vnjudo\" target=\"_blank\">bert-base-chinese</a></strong> to <a href=\"https://wandb.ai/devilhyt/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ad382721d046cb9b4b21dc7aab0f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2029, 'learning_rate': 6.629213483146066e-07, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad649256f6f342dea9c15a9c81adf330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-178\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-178\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0407233238220215, 'eval_accuracy': 0.47692307692307695, 'eval_runtime': 8.3604, 'eval_samples_per_second': 171.045, 'eval_steps_per_second': 10.765, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-178\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-178\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-178\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9702, 'learning_rate': 1.3220973782771534e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ede70c26f5445fd86a190d56b1ae6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-356\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-356\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7164854407310486, 'eval_accuracy': 0.7454545454545455, 'eval_runtime': 8.3749, 'eval_samples_per_second': 170.747, 'eval_steps_per_second': 10.746, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-356\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-356\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-356\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6151, 'learning_rate': 1.98876404494382e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df6c15227e84edc975a7c4aad31f16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-534\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-534\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4260358512401581, 'eval_accuracy': 0.8552447552447553, 'eval_runtime': 8.3859, 'eval_samples_per_second': 170.523, 'eval_steps_per_second': 10.732, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-534\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-534\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-534\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4159, 'learning_rate': 1.7191011235955056e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9765c02a26174f639756729bdb95e99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-712\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-712\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34175896644592285, 'eval_accuracy': 0.8797202797202798, 'eval_runtime': 8.4234, 'eval_samples_per_second': 169.765, 'eval_steps_per_second': 10.685, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-712\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-712\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-712\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3398, 'learning_rate': 1.4349919743178168e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a764555f01b5488389f2190ad8a7a1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-890\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-890\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3345033526420593, 'eval_accuracy': 0.8867132867132868, 'eval_runtime': 8.4682, 'eval_samples_per_second': 168.866, 'eval_steps_per_second': 10.628, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-890\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-890\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-890\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3044, 'learning_rate': 1.1492776886035314e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5723eec19248471c8541ab17deba2f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-1068\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-1068\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33028340339660645, 'eval_accuracy': 0.8867132867132868, 'eval_runtime': 8.3622, 'eval_samples_per_second': 171.008, 'eval_steps_per_second': 10.763, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-1068\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-1068\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-1068\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2758, 'learning_rate': 8.635634028892456e-07, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c796a9eb9b8540b1aafab348f9889705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-1246\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-1246\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29813963174819946, 'eval_accuracy': 0.8986013986013986, 'eval_runtime': 8.438, 'eval_samples_per_second': 169.471, 'eval_steps_per_second': 10.666, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-1246\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-1246\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-1246\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2626, 'learning_rate': 5.778491171749599e-07, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d2c5f40200448aa75c8196503c3ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-1424\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-1424\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2960302233695984, 'eval_accuracy': 0.9041958041958041, 'eval_runtime': 8.3875, 'eval_samples_per_second': 170.492, 'eval_steps_per_second': 10.73, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-1424\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-1424\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-1424\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2506, 'learning_rate': 2.921348314606741e-07, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11fc515c57145bbb3608be21adeefcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-1602\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-1602\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2880263924598694, 'eval_accuracy': 0.9034965034965035, 'eval_runtime': 8.3638, 'eval_samples_per_second': 170.975, 'eval_steps_per_second': 10.761, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-1602\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-1602\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-1602\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2466, 'learning_rate': 6.420545746388443e-09, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017f1ebd45e9429db4d5f5bfe07eb12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\\checkpoint-1780\n",
      "Configuration saved in bert-base-chinese-20220610-5\\checkpoint-1780\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29309555888175964, 'eval_accuracy': 0.9034965034965035, 'eval_runtime': 8.425, 'eval_samples_per_second': 169.734, 'eval_steps_per_second': 10.683, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-base-chinese-20220610-5\\checkpoint-1780\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\checkpoint-1780\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\checkpoint-1780\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-base-chinese-20220610-5\\checkpoint-1602 (score: 0.2880263924598694).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1107.6024, 'train_samples_per_second': 103.268, 'train_steps_per_second': 1.607, 'train_loss': 0.48838024139404296, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1780, training_loss=0.48838024139404296, metrics={'train_runtime': 1107.6024, 'train_samples_per_second': 103.268, 'train_steps_per_second': 1.607, 'train_loss': 0.48838024139404296, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/accuracy.png\" width=\"30%\">\n",
    "<img src=\"./image/loss.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清除Vram中的殘留檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驗證測試集的預測準確度，達到90%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1430\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e436fd1fd1044e80bde4bd525aeaa62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-1.728 , -1.357 ,  3.428 ],\n",
       "       [ 3.666 , -1.156 , -1.995 ],\n",
       "       [-1.134 ,  2.732 , -1.078 ],\n",
       "       ...,\n",
       "       [-2.086 ,  3.137 , -0.5977],\n",
       "       [ 3.66  , -1.509 , -1.566 ],\n",
       "       [ 3.43  , -1.1045, -1.3   ]], dtype=float16), label_ids=array([2, 0, 1, ..., 1, 0, 0], dtype=int64), metrics={'test_loss': 0.30578866600990295, 'test_accuracy': 0.9, 'test_runtime': 8.6411, 'test_samples_per_second': 165.489, 'test_steps_per_second': 10.415})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset=tokenized_datasets[\"test\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "儲存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-base-chinese-20220610-5\n",
      "Configuration saved in bert-base-chinese-20220610-5\\config.json\n",
      "Model weights saved in bert-base-chinese-20220610-5\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-chinese-20220610-5\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-20220610-5\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cf43b20bf8dbdabe236b53da0cab24168cab658c0db4f5c713ea21bee1c0fef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
